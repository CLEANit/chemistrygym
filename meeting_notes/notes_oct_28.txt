Notes:

Attendees:
- Chris
- Sriram
- Isaac
- Mark
- Nouha
- Mitchell

We need simulations, experiments, a framework, and relevant papers.

Chris: No updates, coursework
Nouha: No updates, very relevant coursework
Mitchell: Added the info file, otherwise, coursework
Sriram: Got into the __ cluster, started running experiments; implemented some reinforcements learning, found some issues (for Mitchell to look at), better documentation and README files, one concrete example required
	Unclear in many parts, need use cases, what the repos are used for, this can be directly inputted into the paper, will complete a few other updates, reward function needs to be ironed out (one function for rewards)
	Be able to cap the rewards to a maximum (eliminate infinite values), set up meetings with Sriram, Chris, and Mitchell to look in depth at code base, extraction bench has discrete action space (change to continuous),
	Need uniformity between benches to implement a single RL method (if possible), hash out the purpose and placement of the lab manager for a more concrete implementation, the state is very large (how much of the state is accessible by the agent),
	POMDP vs. MDP (observations cost a lot), new mass spectrometry bench gives a view of the state (at a high cost), currently everything is deterministic but not everything is stochastic (any nonstochasticty is on the human side),
	Deterministic but sensitive to human influence, we need sucinct diagrams that have direct translations into the paper (paper appropriate figures), designate someone to draw the figures (Google Draw, draw.io) (Nouha)
	Config file describing what to run with what parameters to obtain a specific result
	Create a function to take a state and return what the agent observes (observation function)
	Need an algorithm to do both discrete and continuous, or have the lab manager select from two different algorithms depending on if the action space is discrete or continuous
	Keep an open mind about the implementation and capabilities of the lab manager



